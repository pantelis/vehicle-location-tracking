
# Vehicle Detection Project

Pantelis Monogioudis

NOKIA

---

This project detects vehicles using video cameras only. The steps of the processing pipeline project are as follows:

* Apply a color transform and spatial histogram  
* Perform a Histogram of Oriented Gradients (HOG) feature extraction on a labeled training set of images.
* Append the binned colon features to the HOG features. 
* Train a Linear SVM classifier using the combines feature vector
* Implement a sliding-window technique and use your trained classifier to search for vehicles in images.
* Run your pipeline on a video stream and store in a sliding window the binding boxes of detected vehicle segments per frame. Use the groupRectangles function to group clusters of bounding boxes and eliminate outliers.

[//]: # (Image References)
[car_not_car]: ./examples/car_not_car.png
[color_histogram_features]: ./test_images/color_histogram_features.jpg
[cutout1]: ./test_images/cutout1.jpg
[hog_output]: ./test_images/hog_output.jpg
[3d_color_conversion]: ./test_images/3d_color_conversion.png
[detection_output]: ./test_images/detection_output.jpg
[project_video_output]: https://youtu.be/n89TnDt3AQY "Outpout of Project Video"

## Pipeline
The pipeline is driven by a ```config.ini``` file that contains all the configuration parameters and the ```main.py``` that produces all results. 

````python
[Project]
media_type = video
regenerate_features = false
retrain = false

[Classifier]
car_classify_data_dir = ../datasets/udacity-vehicle-tracking
model_filename = svc_vehicle_detection.p
image_format = png
# Colorspace conversion can be None or RGB2: HSV, LUV, HLS, YUV, YCrCb
colorspace_conv = RGB2YCrCb
 
[FeaturesGenerator]
features_filename = features.p
spatial_feat = True
hist_feat = True
hog_feat = True
spatial_size = (32, 32)   
# Number of histogram bins
hist_bins = 32
orient = 9
pix_per_cell = 8
cell_per_block = 2
# HOG channels must be a list of up to 3 elements starting from 0: e.g. [0, 1, 2]
hog_channels = [0,1,2]

[Test]
image_format = jpg
# limits of car detection in the y-axis
y_start_stop = [400, 720]
#
scale = 1.5

# filter span - in frames
filter_span = 10

# group rectangles
group_rectangles_flag = True

# time domain filter
time_domain_filter_flag = True

````

The sections are self evident and their individual parameters control the execution of each step of the pipeline as described below. The other key content in this project is the ``helper.py`` file that contains the vast majority of the functions. 

For the input dataset we have used only the vehicle vs non-vehicle files provided by the GTI and KTTI sources.

### Feature Generation

#### Spatial binning and color histogram extracted features
The feature extraction state of the pipeline is shown below:

````python

        # Read in each one by one
        image = mpimg.imread(file)

        # apply color conversion
        feature_image = convert_color(image, conv=colorspace_conv)

        if spatial_feat:
            spatial_features = bin_spatial(feature_image, size=spatial_size)
            file_features.append(spatial_features)
        if hist_feat:
            # Apply color_hist()
            hist_features = color_hist(feature_image, nbins=hist_bins)
            file_features.append(hist_features)
        if hog_feat:

            hog_features = []

            for channel in literal_eval(hog_channels):
                hog_features.append(get_hog_features(feature_image[:, :, channel], orient, pix_per_cell, cell_per_block,
                                        vis=False, feature_vec=True))
            hog_features = np.ravel(hog_features)

````
Features are generated for each of the two classes: vehicle and not vehicle.  

![alt text][car_not_car]

The input images are (64,64) pixels and after color conversion to ```YCrCb```, the ```bin_spatial()``` function converts the images to (32,32). The converted images are aslo binned by the ```color_hist()``` function that produces a histogram of 32 bins per channel. 
 
The output of the histogram for the picture
 
![cutout1][cutout1]

is shown below. 

![color_histogram_features][color_histogram_features]


#### Histogram of Oriented Gradients (HOG) and Parameters
For the HOG feature extraction step, we have used the paramaters shown in the ```config.ini``` code listing above. The values were the ones suggested as default values. The only exception is the color conversion where YCrCb was selected as it offered better discrimination capability for the test images as shown for example in ```test2.jpg```
 
![3d_color_conversion][3d_color_conversion]

Here is an example generated by the file ```test_plot_hog.py``` using the `YCrCb` color space and HOG parameters of `orientations=9`, `pixels_per_cell=(8, 8)` and `cells_per_block=(2, 2)`:


![hog_output][hog_output]

Note that the ```main.py``` includes the possibility of storing the features as shown below. 

```python


    t10 = time.time()
    if config['Project'].getboolean('regenerate_features'):
        car_images = helper.list_files(os.path.join(car_classify_data_dir, 'vehicles'))
        notcar_images = helper.list_files(os.path.join(car_classify_data_dir, 'non-vehicles'))

        car_features = helper.extract_features(car_images,
                                               colorspace_conv=config['FeaturesGenerator']['colorspace_conv'],
                                               spatial_size=literal_eval(config['FeaturesGenerator']['spatial_size']),
                                               hist_bins=config['FeaturesGenerator'].getint('hist_bins'),
                                               orient=config['FeaturesGenerator'].getint('orient'),
                                               pix_per_cell=config['FeaturesGenerator'].getint('pix_per_cell'),
                                               cell_per_block=config['FeaturesGenerator'].getint('cell_per_block'),
                                               hog_channels=config['FeaturesGenerator']['hog_channels'],
                                               spatial_feat=config['FeaturesGenerator'].getboolean('spatial_feat'),
                                               hist_feat=config['FeaturesGenerator'].getboolean('hist_feat'),
                                               hog_feat=config['FeaturesGenerator'].getboolean('hog_feat'))

        notcar_features = helper.extract_features(notcar_images,
                                                  colorspace_conv=config['FeaturesGenerator']['colorspace_conv'],
                                                  spatial_size=literal_eval(config['FeaturesGenerator']['spatial_size']),
                                                  hist_bins=config['FeaturesGenerator'].getint('hist_bins'),
                                                  orient=config['FeaturesGenerator'].getint('orient'),
                                                  pix_per_cell=config['FeaturesGenerator'].getint('pix_per_cell'),
                                                  cell_per_block=config['FeaturesGenerator'].getint('cell_per_block'),
                                                  hog_channels=config['FeaturesGenerator']['hog_channels'],
                                                  spatial_feat=config['FeaturesGenerator'].getboolean('spatial_feat'),
                                                  hist_feat=config['FeaturesGenerator'].getboolean('hist_feat'),
                                                  hog_feat=config['FeaturesGenerator'].getboolean('hog_feat'))

        features = {}
        features["car"] = car_features
        features["notcar"] = notcar_features
        features["feature_generator_parameters"] = {
            'spatial_feat': config['FeaturesGenerator']['spatial_feat'],
            'hist_feat': config['FeaturesGenerator']['hist_feat'],
            'hog_feat': config['FeaturesGenerator']['hog_feat'],
            'colorspace_conv': config['FeaturesGenerator']['colorspace_conv'],
            'spatial_size': config['FeaturesGenerator']['spatial_size'],
            'hist_bins': config['FeaturesGenerator']['hist_bins'],
            'orient': config['FeaturesGenerator']['orient'],
            'pix_per_cell': config['FeaturesGenerator']['pix_per_cell'],
            'cell_per_block': config['FeaturesGenerator']['cell_per_block'],
            'hog_channels': config['FeaturesGenerator']['hog_channels']}

        joblib.dump(features, open(os.path.join(car_classify_data_dir, features_filename), 'wb'), protocol=pickle.HIGHEST_PROTOCOL)

    else:
        # read the car features from the pickled dict
        features = joblib.load(open(os.path.join(car_classify_data_dir, features_filename), 'rb'))

        car_features = features['car']
        notcar_features = features['notcar']

        feature_generator_parameters = features['feature_generator_parameters']
        feature_generator_parameters = features['feature_generator_parameters']
        orient = int(feature_generator_parameters["orient"])
        pix_per_cell = int(feature_generator_parameters["pix_per_cell"])
        cell_per_block = int(feature_generator_parameters["cell_per_block"])
        spatial_size = literal_eval(feature_generator_parameters["spatial_size"])
        hist_bins = int(feature_generator_parameters["hist_bins"])
        spatial_feat = bool(feature_generator_parameters['spatial_feat'])
        hist_feat = bool(feature_generator_parameters['hist_feat'])
        hog_feat = bool(feature_generator_parameters['hog_feat'])
        
```
#### SVM Classifier Training 
The SVM classifier was trained using the extracted and stored features as shown below.

```python

 # Training
    if config['Project'].getboolean('retrain'):

        # Create an array stack of feature vectors
        X = np.vstack((car_features, notcar_features)).astype(np.float64)

        # Fit a per-column scaler
        X_scaler = StandardScaler().fit(X)

        # Apply the scaler to X
        scaled_X = X_scaler.transform(X)

        # Define the labels vector
        y = np.hstack((np.ones(len(car_features)), np.zeros(len(notcar_features))))

        # Split up data into randomized training and test sets
        rand_state = np.random.randint(0, 100)
        X_train, X_test, y_train, y_test = train_test_split(
            scaled_X, y, test_size=0.2, random_state=rand_state)

        # print('Feature vector length:', len(X_train[0]))

        # Use a linear SVC
        svc = LinearSVC()

        # Check the training time for the SVC
        t30 = time.time()

        svc.fit(X_train, y_train)
        model = {'svc': svc, 'X_scaler': X_scaler}

        t40 = time.time()
        print(round(t40 - t30, 2), 'Seconds to train SVC...')

        # save the model to disk
        joblib.dump(model, open(os.path.join(car_classify_data_dir, model_filename), 'wb'), protocol=pickle.HIGHEST_PROTOCOL)

        # Check the score of the SVC
        print('Test Accuracy of SVC = ', round(svc.score(X_test, y_test), 4))
```
Randomization of test and training datasets was done and a the linear SVM achieved 99.75% accuracy on the test dataset (20% of the dataset). Both SVM and Scaler models are also stored to avoid re-running the training at every experimental step. 

#### Sliding Window Search
The sliding window search is shown below. As intuitively can be understood we limited the search to the bottom section of the frame as manifested by the ```y_start_stop = [400, 720]``` parameter. The cell size was selected to be (8,8) pixels and the block size was (2,2) cells. The overlap was selected at 50%. 

````python

def slide_window(img, x_start_stop=[None, None], y_start_stop=[None, None],
                    xy_window=(64, 64), xy_overlap=(0.5, 0.5)):
    # If x and/or y start/stop positions not defined, set to image size
    if x_start_stop[0] == None:
        x_start_stop[0] = 0
    if x_start_stop[1] == None:
        x_start_stop[1] = img.shape[1]
    if y_start_stop[0] == None:
        y_start_stop[0] = 0
    if y_start_stop[1] == None:
        y_start_stop[1] = img.shape[0]
    # Compute the span of the region to be searched
    xspan = x_start_stop[1] - x_start_stop[0]
    yspan = y_start_stop[1] - y_start_stop[0]
    # Compute the number of pixels per step in x/y
    nx_pix_per_step = np.int(xy_window[0]*(1 - xy_overlap[0]))
    ny_pix_per_step = np.int(xy_window[1]*(1 - xy_overlap[1]))
    # Compute the number of windows in x/y
    nx_buffer = np.int(xy_window[0]*(xy_overlap[0]))
    ny_buffer = np.int(xy_window[1]*(xy_overlap[1]))
    nx_windows = np.int((xspan-nx_buffer)/nx_pix_per_step)
    ny_windows = np.int((yspan-ny_buffer)/ny_pix_per_step)
    # Initialize a list to append window positions to
    window_list = []
    # Loop through finding x and y window positions
    # Note: you could vectorize this step, but in practice
    # you'll be considering windows one by one with your
    # classifier, so looping makes sense
    for ys in range(ny_windows):
        for xs in range(nx_windows):
            # Calculate window position
            startx = xs*nx_pix_per_step + x_start_stop[0]
            endx = startx + xy_window[0]
            starty = ys*ny_pix_per_step + y_start_stop[0]
            endy = starty + xy_window[1]
            # Append window position to list
            window_list.append(((startx, starty), (endx, endy)))
    # Return the list of windows
    return window_list
````
By configuring the ```config.ini``` such that we perform vehicle detection on test images instead of the project video, we can get the following images that show the positive detections in 6 test images. You can see multiple overlapping windows placed in the two cars. In most cases the darker colored car attracted more detection windows than white car while at the same time in one occasion a false alarm can also be observed since a vehicle was detected in the opposite stream. 
 
![detection_output][detection_output]


### Video Implementation

https://youtu.be/
[![project_video_output](https://img.youtube.com/vi/n89TnDt3AQY/0.jpg)](https://youtu.be/n89TnDt3AQY)

( Click to the image above for the video. )


I recorded the positions of positive detections in each frame of the video. A different method than heatmap generation was adopted. In our method the multiple positive windows are fed into a ```deque``` data structure. The structure stored the latest ```filter_span``` frames. All positive windows across the ```filter_span=10``` frames are input to the ```cv2.groupRectangles``` function. This function forms clusters depending on the setting of its parameters. We have determined that the optimal set of parameters are as follows:

* groupThreshold = 1. This is the minimum possible number of rectangles minus 1 that need to be present for the grouping to take place. So for example if at least two positives windows per vehicle are present, these windows will be retained and the output will be the **average** window. As a result, unless false detections are persistent, spurious false positives will be eliminated from the output. The video produced a very small number of false positives. 
* eps = 0.2. This is the relative difference between sides of the rectangles to merge them into a group. 
For example, if eps=1.0, then clusters of rectangles which effectively are associated with corresponding cars will be merged. This is not desired as each cluster must correspond to one vehicle. In contrast, when eps=0 then all rectangles will be retained and no merging will be performed. 
The choice of 0.2 has been shown to result in a good tradeoff by experimentation. 

---

### Discussion
The adopted method can be improved significantly as follows by tracking. No concept of memory is present in this implementation other than a time domain filter. One can take the centroids of the positive bounding boxes and based on a distance criterion determine the cluster centers. Merging the centroids without a-priori knowledge of the cluster centers can be done using affinity propagation. The number of exemplars will vary as the number of cars that can be observed vary as well. For example, in the beginning of the video the two close by cars are distinct but then one ocludes the other. In such case there are two examplars in the beginning, then a single examplar and then two exemplars once again. Towards the very end of the video there are three exemplars in fact - as a third car appears in the far right lane. The resulting variable number of centroids can be tracked using a more advanced filter such as a Kalman Filter. As a result, the rectangles are appearing wobbly. 
  
2. There is more training work that is needed for bright colored vehicles. There seems to be a bias in the dataset towards darker colored vehicles. Its a problem that can be addressed by more carefully selecting the training dataset. 


